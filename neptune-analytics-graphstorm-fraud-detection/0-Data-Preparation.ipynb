{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare graph data\n",
    "\n",
    "## Required Dependencies\n",
    "Before running this notebook, ensure you have the following packages installed:\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "Required Python version: >= 3.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook you will start with creating a Neptune Analytics graph and then exporting that as CSV. You will then use the NeptuneGS library to analyze the exported data and create a GraphStorm graph processing configuration file that you will use to kickstart the GraphStorm learning pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import example data into a new Neptune Analytics graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When starting with an existing Neptune Analytics graph you will be able to directly export it and start from the optional 'Export graph from Neptune Analytics' step.\n",
    "\n",
    "In this case our data processing script will prepare the data that emulates the Neptune Analytics export schema in the interest of time.\n",
    "\n",
    "Note: The dataset contains approximately 3.5% fraudulent transactions, making it an imbalanced classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by cloning the GraphStorm repository, which you will use later to launch SageMaker jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GS_HOME=\"~/graphstorm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/awslabs/graphstorm.git $GS_HOME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, force=True)\n",
    "logging.getLogger(\"boto3\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"botocore\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"urllib3\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"s3transfer\").setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# Configure boto3 client with retries and error handling\n",
    "from botocore.config import Config\n",
    "config = Config(\n",
    "    retries={\"total_max_attempts\": 1, \"mode\": \"standard\"},\n",
    "    read_timeout=None\n",
    ")\n",
    "neptune_graph = boto3.client(\"neptune-graph\", config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variables that will be used across all notebooks,\n",
    "# Replace BUCKET value with your S3 bucket\n",
    "BUCKET=\"<YOUR_BUCKET_HERE>\"\n",
    "\n",
    "GRAPH_NAME=\"ieee-cis-fraud-detection\"\n",
    "AWS_REGION=\"us-east-1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Neptune Graph\n",
    "\n",
    "Start by creating a Neptune Graph in the background while you work through the rest of the notebooks. You will use this graph to import the graph data after you have enriched them with GNN embeddings and predictions.\n",
    "\n",
    "### Required IAM Permissions\n",
    "Your IAM role needs the following permissions:\n",
    "- AWSNeptuneAnalyticsFullAccess\n",
    "- AmazonS3FullAccess (or more restricted S3 access)\n",
    "- AWS KMS permissions if using encrypted S3 buckets\n",
    "\n",
    "For detailed permissions and trust policy requirements, see:\n",
    "- [Neptune Analytics IAM Roles](https://docs.aws.amazon.com/neptune-analytics/latest/userguide/security-iam.html)\n",
    "- [Import/Export Permissions](https://docs.aws.amazon.com/neptune-analytics/latest/userguide/import-export-permissions.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The embeddings size needs to be set at graph creation time\n",
    "EMBEDDING_SIZE = 128\n",
    "\n",
    "try:\n",
    "    # Create a Neptune Analytics graph\n",
    "    create_response = neptune_graph.create_graph(\n",
    "        graphName=GRAPH_NAME,\n",
    "        deletionProtection=False,\n",
    "        publicConnectivity=True,\n",
    "        vectorSearchConfiguration={\"dimension\": EMBEDDING_SIZE},\n",
    "        replicaCount=0,\n",
    "        provisionedMemory=16,\n",
    "    )\n",
    "    # Make a note of the graph ID for later use\n",
    "    GRAPH_ID = create_response[\"id\"]\n",
    "except Exception as e:\n",
    "    print(f\"Error creating graph: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert raw data to Neptune Analytics export format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step you will convert the raw graph data into a format that matches the export format of Neptune Analytics. This way you can proceed with \n",
    "the rest of the GNN pipeline while the Neptune Analytics graph is being created. The data conversion will take around 2 minutes to run, after which you will analyze the produced files to extract the graph schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the raw data from the SageMaker examples S3 bucket\n",
    "!aws s3 sync \"s3://sagemaker-solutions-us-west-2/Fraud-detection-in-financial-networks/data\" ./input-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph_data_preprocessor_neptune import create_neptune_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROCESSED_PREFIX = f\"s3://{BUCKET}/neptune-input/{GRAPH_NAME}\"\n",
    "\n",
    "ID_COLS = \"card1,card2,card3,card4,card5,card6,ProductCD,addr1,addr2,P_emaildomain,R_emaildomain\"\n",
    "CAT_COLS = \"M1,M2,M3,M4,M5,M6,M7,M8,M9\"\n",
    "\n",
    "create_neptune_data(\n",
    "    data_prefix=\"./input-data/\",\n",
    "    output_prefix=PROCESSED_PREFIX,\n",
    "    id_cols=ID_COLS,\n",
    "    cat_cols=CAT_COLS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script you just run will create the graph data under `<PROCESSED_PREFIX>/graph_data` and separate train/validation/test splits of the data under `<PROCESSED_PREFIX>/graph_data/data_splits`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Optional] How to import and export a Neptune Analytics graph\n",
    "\n",
    "In this section you will learn how you could import and export a Neptune Analytics graph using boto3. You don't need to run the code for this example as we already create the data in a compatible format, but in case you want to test out the import/export functionality yourself, you can follow the steps below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import graph example\n",
    "\n",
    "You can create a new graph and import the data you created above using the following boto call. You will need to set up a Neptune import role, with a trust relationship that allows the Neptune Analytics service to assume it, and a the role will need S3 read access for your data source.\n",
    "\n",
    "For details see https://docs.aws.amazon.com/neptune-analytics/latest/userguide/bulk-import-create-from-s3.html\n",
    "\n",
    "```python\n",
    "\n",
    "NEPTUNE_IMPORT_ROLE = \"arn:aws:iam::012345678912:role/NeptuneAnalyticsImportRole\"\n",
    "\n",
    "# You need to provide the embedding size in advance if you plan to import GraphStorm embeddings later\n",
    "EMBEDDING_SIZE = 128\n",
    "GRAPH_IMPORT_S3 = f\"{PROCESSED_PREFIX}/graph_data\"\n",
    "# Create a neptune analytics import task\n",
    "create_response = neptune_graph.create_graph_using_import_task(\n",
    "    source=GRAPH_IMPORT_S3,\n",
    "    graphName=os.environ['GRAPH_NAME'],\n",
    "    format=\"CSV\",\n",
    "    roleArn=NEPTUNE_IMPORT_ROLE,\n",
    "    deletionProtection=False,\n",
    "    publicConnectivity=True,\n",
    "    vectorSearchConfiguration={\"dimension\": EMBEDDING_SIZE},\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This import task would take around 20 minutes to complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "# Wait for import task to finish\n",
    "GRAPH_ID = create_response[\"graphId\"]\n",
    "IMPORT_TASK_ID = create_response[\"taskId\"]\n",
    "import_waiter = neptune_graph.get_waiter(\"import_task_successful\")\n",
    "import_waiter.wait(taskIdentifier=IMPORT_TASK_ID)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export graph example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the graph with GraphStorm, you would first need to export it to a tabular representation, using NA's `StartExportTask` API. This would be your normal starting point if you already have a graph on Neptune Analytics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need an export role with a trust relationship for the Neptune Analytics service and the ability to write to your intentended output S3 location. See https://docs.aws.amazon.com/neptune-analytics/latest/userguide/exporting-data.html for details on the export capabilities of Neptune Analytics.\n",
    "\n",
    "You will also need a KMS key available that Neptune Analytics will use to encrypt the data during export. The export role will need to be added as one of the users of the KMS key to allow the role to encrypt the data using the key. For a walkthrough on how to do this see\n",
    "https://docs.aws.amazon.com/neptune-analytics/latest/userguide/import-export-permissions.html#create-iam-and-kms\n",
    "\n",
    "```python\n",
    "\n",
    "EXPORT_PREFIX = f\"s3://{os.environ['BUCKET']}/neptune-export/{os.environ['GRAPH_NAME']}/\"\n",
    "KMS_KEY_ARN = (\n",
    "    \"arn:aws:kms:us-east-1:012345678912:key/xxxxxxx-kms-key-arn\"\n",
    ")\n",
    "# Export role needs to be able to use the KMS key to encrypt data\n",
    "NEPTUNE_EXPORT_ROLE = \"arn:aws:iam::012345678912:role/NeptuneAnalyticsExportRole\"\n",
    "\n",
    "export_response = neptune_graph.start_export_task(\n",
    "    destination=EXPORT_PREFIX,\n",
    "    graphIdentifier=GRAPH_ID,\n",
    "    roleArn=NEPTUNE_EXPORT_ROLE,\n",
    "    kmsKeyIdentifier=KMS_KEY_ARN,\n",
    "    format=\"CSV\",\n",
    ")\n",
    "# Assign the export task id to a variable\n",
    "EXPORT_TASK_ID = export_response[\"taskId\"]\n",
    "print(EXPORT_TASK_ID)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The export process should take 10-20 minutes.\n",
    "\n",
    "```python\n",
    "# Wait for export task to complete\n",
    "export_waiter = neptune_graph.get_waiter(\"export_task_successful\")\n",
    "export_waiter.wait(taskIdentifier=EXPORT_TASK_ID)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create GConstruct configuration file from exported data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GraphStorm training requires the original graph data to be converted into a binary, partitioned graph representation to support efficient distributed training. GraphStorm provides the GConstruct module and GSProcessing library that can accomplish this on a single instance or distributed respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create the train data for GraphStorm we need to create a JSON file that describes the tabular graph data from the NA export task. An example file can be:\n",
    "\n",
    "```json\n",
    "\n",
    "{\n",
    "    \"nodes\": [\n",
    "        {\n",
    "            \"node_id_col\":  \"nid\",\n",
    "            \"node_type\":    \"paper\",\n",
    "            \"format\":       {\"name\": \"parquet\"},\n",
    "            \"files\":        [\"paper_nodes.parquet\"],\n",
    "            \"features\":     [\n",
    "                {\n",
    "                    \"feature_col\":  \"embedding\"\n",
    "                }\n",
    "            ],\n",
    "            \"labels\": [   \n",
    "                {\n",
    "                    \"label_col\":    \"paper_field\",\n",
    "                    \"task_type\":    \"classification\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    \"edges\": [\n",
    "        {\n",
    "            \"source_id_col\":    \"src\",\n",
    "            \"dest_id_col\":      \"dst\",\n",
    "            \"relation\":         [\"paper\", \"cites\", \"paper\"],\n",
    "            \"format\":           {\"name\": \"parquet\"},\n",
    "            \"files\":            [\"paper_cites_paper_edges.parquet\"]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To create such a configuration for the NA-exported data you can use the `neptune_gs` package from the repository. The package includes a script that analyzes the output, gets user input to clarify relations and features when needed, and creates the GConstruct configuration JSON file. \n",
    "\n",
    "The package is available at the top level of the repository under the `neptune-gs` directory and you should have installed it during the pre-requisites phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create GConstruct config\n",
    "\n",
    "The `create_graphstorm_config` function analyzes the Neptune Export data, optionally asks a series of questions to determine the desired graph schema, and creates a JSON file GConstruct will use as input. The default filename is `gconstruct_config.json`.\n",
    "\n",
    "The program will iterate through all columns for all vertex and relation files, and provide default transformations for each column, provided on whether it's a feature or label.\n",
    "\n",
    "In case it's not able to automatically determine some feature type or edge triplets it will as for input by the user. **For existing Neptune Analytics graphs, providing the graph identifier can help with automatically extracting this information.**\n",
    "\n",
    " > NOTE: Because for this example we don't have graph data already imported in Neptune Analytics, we have named the edge files in a way that the files correspond to edge triplets, e.g. `Transaction,identified_by,Card1` fully determines an edge triple. The script relies on this setup to automatically detect edge triples without user input. Otherwise, providing the `graph_id` to the `create_graphstorm_config` function ensures you can extract all edge types from the Neptune graph itself, without user input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "from neptune_gs.create_gconstruct import create_graphstorm_config\n",
    "\n",
    "\n",
    "EXPORTED_GRAPH_S3 = osp.join(PROCESSED_PREFIX, \"graph_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPORTED_GRAPH_S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_config = create_graphstorm_config(\n",
    "    EXPORTED_GRAPH_S3,\n",
    "    # graph_id=GRAPH_ID, # For this example we don't use the graph ID\n",
    "    learning_task=\"classification\", # The task is node classification\n",
    "    target_type=\"Transaction\", # The target node type are Transaction nodes\n",
    "    label_column=\"isFraud:Int\",  # The property of the Transaction nodes we want to predict. Column type (Int) is appended by Neptune during CSV export\n",
    "    masks_prefix=osp.join(EXPORTED_GRAPH_S3, \"data_splits\"), # The location of the train/validation/test masks\n",
    "    cols_to_keep_dict={ # Select a subset of the Transaction properties to include\n",
    "        \"Transaction\":\n",
    "            # Required columns\n",
    "            [\n",
    "                \"~id\",\n",
    "                \"~label\",\n",
    "                \"isFraud:Int\",\n",
    "            ]\n",
    "            +\n",
    "            # Numerical features without missing values\n",
    "            [f\"C{idx}:Float\" for idx in range(1, 15)] + [\"TransactionAmt:Float\"]\n",
    "            +\n",
    "            # Categorical features\n",
    "            [f\"{CAT_COL}:String\" for CAT_COL in CAT_COLS.split(\",\")]\n",
    "    },\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you provide all values the function returns a GraphStorm graph construction configuration dict which you can save to a JSON file. For more information see the documentation about [how GraphStorm performs graph construction](https://graphstorm.readthedocs.io/en/latest/cli/graph-construction/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the config dict in a readable format\n",
    "from pprint import pp\n",
    "\n",
    "pp(gs_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting feature list from GConstruct to use during training\n",
    "\n",
    "\n",
    "To include node/edge features during training and inference, GraphStorm needs a list of all node/edge types with features and the feature names. \n",
    "For this example we are providing a YAML file that already contains all the configuration needed to run a node classification task.\n",
    "\n",
    "If you were writing the YAML file yourself however, you can use \n",
    "the following convenience function to extract the lists of features\n",
    "for every node type to include in your yaml file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neptune_gs.create_gconstruct import extract_features_from_gconstruct_config\n",
    "\n",
    "node_feature_lists, edge_feature_lists = extract_features_from_gconstruct_config(\n",
    "    gs_config\n",
    ")\n",
    "node_feature_lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example we aggregated all the individual numerical features in one feature vector per feature type. This helps with processing the graph data faster downstream."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write graph construction configuration file to S3\n",
    "\n",
    "To be able to build a partitioned graph from the graph data on S3 you need to save the configuration dictionary as a JSON file, under the same path as the exported data. \n",
    "\n",
    "`neptune_gs` provides the `FileSystemHandler` class for easier reading and writing from and to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os.path as osp\n",
    "\n",
    "from neptune_gs.fs_handler import FileSystemHandler\n",
    "\n",
    "fs_handler = FileSystemHandler(EXPORTED_GRAPH_S3)\n",
    "CONFIG_FILENAME = \"ieee-cis-gconstruct-node-classification.json\"\n",
    "input_no_protocol: str = osp.join(\n",
    "    EXPORTED_GRAPH_S3.replace(\"s3://\", \"\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the config locally and at the input location\n",
    "with open(CONFIG_FILENAME, \"w\") as f:\n",
    "    json.dump(gs_config, f, indent=2)\n",
    "\n",
    "with fs_handler.pa_fs.open_output_stream(\n",
    "    f\"{osp.join(input_no_protocol, CONFIG_FILENAME)}\"\n",
    ") as f:\n",
    "    f.write(json.dumps(gs_config, indent=2).encode(\"utf-8\"))\n",
    "\n",
    "print(\n",
    "    f\"GRAPH_NAME: {GRAPH_NAME}\"\n",
    "    f\"\\nConfiguration written to ./{CONFIG_FILENAME}\"\n",
    "    f\"\\nand to s3://{osp.join(input_no_protocol, CONFIG_FILENAME)}\"\n",
    ")\n",
    "\n",
    "# Let's also write information about the graph and exports locally\n",
    "with open(\"task-info.json\", \"w\") as f:\n",
    "    export_info = {\n",
    "        \"AWS_REGION\": AWS_REGION,\n",
    "        \"BUCKET\": BUCKET,\n",
    "        \"EXPORTED_GRAPH_S3\": EXPORTED_GRAPH_S3,\n",
    "        \"GCONSTRUCT_CONFIG\": CONFIG_FILENAME,\n",
    "        \"NODE_FEATURE_LISTS\": node_feature_lists,\n",
    "        \"EDGE_FEATURE_LISTS\": edge_feature_lists,\n",
    "        \"GRAPH_ID\": GRAPH_ID,\n",
    "        \"GRAPH_NAME\": GRAPH_NAME,\n",
    "        \"GS_HOME\": GS_HOME,\n",
    "    }\n",
    "    json.dump(export_info, f, indent=2)\n",
    "print(\"Task info written to ./task-info.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next notebook you will prepare your SageMake environment for training with GraphStorm, building and pushing the necessary Docker images."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
