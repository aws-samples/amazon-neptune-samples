{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy and execute a GraphStorm SageMaker Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you can create the SageMaker pipeline to run the steps needed to train a GraphStorm model. GraphStorm provides helper scripts to create a SageMaker pipeline. To run the example, you would need to have cloned the GraphStorm repository to get access to the pipeline creation and execution scripts, which you should have done in notebook 0, with `git clone https://github.com/awslabs/graphstorm.git $HOME/graphstorm-demo`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, force=True)\n",
    "logging.getLogger(\"boto3\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"botocore\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"aibotocore\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"fsspec\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"urllib3\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"s3transfer\").setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare and execute pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As first step, extract the information for the data outputs you created in notebook 0-Data-Preparation. You will use this information to set up your SageMaker GraphStorm pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pp\n",
    "\n",
    "# Read information about the graph from the JSON file you created in notebook 0\n",
    "with open(\"task-info.json\", \"r\") as f:\n",
    "    task_info = json.load(f)\n",
    "pp(task_info)\n",
    "\n",
    "GRAPH_NAME=task_info[\"GRAPH_NAME\"]\n",
    "BUCKET=task_info[\"BUCKET\"]\n",
    "GRAPH_ID=task_info[\"GRAPH_ID\"]\n",
    "EXPORTED_GRAPH_S3 = task_info[\"EXPORTED_GRAPH_S3\"]\n",
    "GCONSTRUCT_CONFIG = task_info[\"GCONSTRUCT_CONFIG\"]\n",
    "GS_HOME = task_info[\"GS_HOME\"]\n",
    "\n",
    "# The pipeline's SageMaker execution role needs the following permissions:\n",
    "# - AmazonSageMakerFullAccess\n",
    "# - AmazonS3FullAccess (or more restricted S3 access as needed, needs read/write to your $BUCKET)\n",
    "# - AWS KMS permissions if using encrypted S3 buckets\n",
    "# For detailed permissions, see: https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html\n",
    "SM_EXECUTION_ROLE = \"arn:aws:iam::123456789012:role/service-role/<Your-SageMaker-Job-Execution-Role>\"\n",
    "\n",
    "\n",
    "TRAIN_YAML = \"ieee_cis_nc_training.yaml\"\n",
    "INFERENCE_YAML = \"ieee_cis_nc_inference.yaml\"\n",
    "YAML_S3_PREFIX = f\"s3://{BUCKET}/yaml/{GRAPH_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a name for the pipeline, needs to be unique per region\n",
    "PIPELINE_NAME = f\"{GRAPH_NAME}-graphstorm-pipeline-{GRAPH_ID}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to have the training and inference YAML files available on S3 as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp $TRAIN_YAML $YAML_S3_PREFIX/$TRAIN_YAML\n",
    "!aws s3 cp $INFERENCE_YAML $YAML_S3_PREFIX/$INFERENCE_YAML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training and inference files are mostly the same. The main difference for the inference configuration is that it enables running inference on all target nodes using `infer_all_target_nodes: true`.\n",
    "\n",
    "This produces embeddings for all `Transaction` nodes. To avoid biased evaluation when including nodes from the training set, we also need to disable running evaluation, setting `no_validation: true` for inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make pipeline creation easier, we provide a bash script, `deploy_recommendations_pipeline.sh`, that fills in some of the parameters needed to deploy a pipeline. \n",
    "\n",
    "You will need to provide your input data location (`--input-s3`) and YAML file to use for training and inference (`--yaml-s3`). \n",
    "\n",
    "The script will also require providing a SageMaker execution role ARN configured with S3 access. \n",
    "\n",
    "> **Note about instance type quotas**: The default configuration uses `ml.m5.4xlarge` instances. Your AWS account may have a default quota of 0 for this instance type. If you encounter quota issues, you have several options:\n",
    "> 1. [Request a quota increase through the AWS Service Quotas console](https://docs.aws.amazon.com/sagemaker/latest/dg/regions-quotas.html#regions-quotas-quotas)\n",
    "> 2. Use alternative instance types with similar specifications, for which you already have quotas for.\n",
    ">\n",
    ">To modify the instance type, provide the `--instance-type` argument to the `deploy_fraud_detection_pipeline.sh` script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once all parameters are provided, the script will deploy the SageMaker pipeline. For more details on how to deploy GraphStorm SageMaker pipelines, see the [GraphStorm documentation](https://graphstorm.readthedocs.io/en/latest/advanced/sagemaker-pipelines.html), and contents of `deploy_fraud_detection_pipeline.sh` as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash deploy_fraud_detection_pipeline.sh \\\n",
    "    --bucket-name $BUCKET \\\n",
    "    --config-filename $GCONSTRUCT_CONFIG \\\n",
    "    --execution-role $SM_EXECUTION_ROLE \\\n",
    "    --graphstorm-location $GS_HOME \\\n",
    "    --input-s3 $EXPORTED_GRAPH_S3 \\\n",
    "    --pipeline-name $PIPELINE_NAME \\\n",
    "    --train-yaml-s3 $YAML_S3_PREFIX/$TRAIN_YAML \\\n",
    "    --inference-yaml-s3 $YAML_S3_PREFIX/$INFERENCE_YAML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will create a SageMaker pipeline named `f\"{GRAPH_NAME}-graphstorm-pipeline-{GRAPH_ID}\"`, which you can inspect by navigating to the **[SageMaker Studio UI](https://us-east-1.console.aws.amazon.com/sagemaker/home?region=us-east-1#/studio-landing)**, selecting your Domain and user profile that you used when deploying the pipeline, and selecting **Open Studio**. \n",
    "\n",
    "In the navigation pane, choose **Pipelines**. There should be a pipeline named `ieee-cis-fraud-detection-graphstorm-pipeline-<GRAPH_ID>`. Choose the pipeline, which will take you to the **Executions** tab for the pipeline. Choose **Graph** to view the pipeline steps.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/sm-pipeline.png\" width=\"25%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GraphStorm pipeline has the following steps:\n",
    "\n",
    "* GConstruct that takes the input tabular graph data, processes node identifiers, features and labels, and partitions the graph to prepare\n",
    "it for distributed training\n",
    "* Training step that trains the classification model and produces the output model and any learnable embeddings on S3\n",
    "* An inference step that produces the GNN embeddings and predictions for every target node type (`Transaction`) in the graph and uploads them to S3\n",
    "\n",
    "To execute the pipeline you can run:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a specific subpath to have for the execution\n",
    "import time\n",
    "\n",
    "timestamp = f\"{time.time():.0f}\"\n",
    "EXECUTION_SUBPATH = f\"gs-pipeline-example-{timestamp}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 $GS_HOME/sagemaker/pipeline/execute_sm_pipeline.py \\\n",
    "    --pipeline-name $PIPELINE_NAME \\\n",
    "    --region us-east-1 \\\n",
    "    --execution-subpath $EXECUTION_SUBPATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The execution will take around 25 minutes. You can monitor the progress of the execution in SageMaker Studio Pipelines UI, in the **Executions** tab for the pipeline.\n",
    "\n",
    "Once complete the pipeline will create embeddings for `Transaction` nodes in a pre-determined location on S3, defined in the `deploy_recommendations_pipeline.sh` script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_S3 = f\"s3://{BUCKET}/pipelines-output/{PIPELINE_NAME}/{EXECUTION_SUBPATH}/inference/embeddings/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls $EMB_S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will see that the inference job has created embedding files for every Transaction node in the graph, as well as a JSON file that describes the generated embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRED_S3 = f\"s3://{BUCKET}/pipelines-output/{PIPELINE_NAME}/{EXECUTION_SUBPATH}/inference/predictions/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls $PRED_S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enrich exported graph files with GNN embeddings\n",
    "\n",
    "With the GNN node embeddings now available, you can enrich your original graph data with the GNN embeddings. To do so you can again use a function from the `NeptuneGS` library, providing the path to your original exported data, and the location of the embeddings on S3:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path as osp\n",
    "from neptune_gs.attach_gs_data import attach_gs_data_to_na\n",
    "\n",
    "# Ensure that the enriched files are created under the same prefix as the original data\n",
    "ENRICHED_S3 = osp.join(EXPORTED_GRAPH_S3, \"enriched\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attach_gs_data_to_na(\n",
    "    input_s3=EXPORTED_GRAPH_S3,\n",
    "    output_s3=ENRICHED_S3,\n",
    "    embeddings_s3=EMB_S3,\n",
    "    predictions_s3=PRED_S3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above script will join your original graph with the generated embeddings and predictions based on node id, and create new node output files that include new columns named `embeddings:Vector` and `pred:Float[]` that you will use in the following notebooks to enrich the graph data.\n",
    "\n",
    "By placing the enriched data under the same prefix as the original data, we can import all data using a single [Neptune Analytics import task](https://docs.aws.amazon.com/neptune-analytics/latest/userguide/loading-data-existing-graph.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
