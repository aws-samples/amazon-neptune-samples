{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up SageMaker images for training and processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook you will build and push the Docker images that are needed to run graph processing and training tasks with GraphStorm on SageMaker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "First, let's set up environment variables that will be used across all notebooks in this demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Read information about the graph from the JSON file you created in notebook 0\n",
    "with open(\"task-info.json\", \"r\") as f:\n",
    "    task_info = json.load(f)\n",
    "\n",
    "\n",
    "GRAPH_NAME=task_info[\"GRAPH_NAME\"]\n",
    "BUCKET=task_info[\"BUCKET\"]\n",
    "GS_HOME=task_info[\"GS_HOME\"]\n",
    "AWS_REGION=task_info[\"AWS_REGION\"]\n",
    "GRAPH_ID=task_info[\"GRAPH_ID\"]\n",
    "AWS_REGION=task_info[\"AWS_REGION\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example GraphStorm-SageMaker architecture\n",
    "\n",
    "A common model development process is to perform model exploration locally on a subset of your full data, and when you’re satisfied with the results, train the full-scale model. This setup allows for cheaper exploration before training on the full dataset. \n",
    "\n",
    "We demonstrate such a setup in the following diagram, where a user can perform model development and initial training on a single EC2 instance, and when they’re ready to train on their full data, hand off the heavy lifting to SageMaker for distributed training. Using SageMaker Pipelines to train models provides several benefits, like reduced costs, auditability, and lineage tracking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/sm-graphstorm-arch.jpg\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and Push GraphStorm Docker Images\n",
    "\n",
    "GraphStorm uses BYOC (Bring Your Own Container) to run SageMaker jobs. First you will build the image that you will use to partition the graph and run training and inference.\n",
    "\n",
    "### Required IAM Permissions\n",
    "To build and push the GraphStorm images, your IAM role needs the following permissions:\n",
    "- Pull images from the SageMaker public ECR registry\n",
    "- Create a repository and push images to your account's private ECR registry\n",
    "- For detailed permissions, refer to the [ECR IAM id-based policy examples](https://docs.aws.amazon.com/AmazonECR/latest/userguide/security_iam_id-based-policy-examples.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> NOTE: GraphStorm image builder does not support cross-platform builds (e.g. building linux image on Mac silicon/aarch64), ensure you are building the image on `linux/x86_64` host."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will create an ECR repository and push an image to\n",
    "# ${ACCOUNT_ID}.dkr.ecr.us-east-1.amazonaws.com/graphstorm:sagemaker-cpu\n",
    "!bash $GS_HOME/docker/build_graphstorm_image.sh --environment sagemaker --device cpu\n",
    "!bash $GS_HOME/docker/push_graphstorm_image.sh -e sagemaker -d cpu -r $AWS_REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neptune Analytics Graph Notebook Setup (Optional)\n",
    "\n",
    "This section is optional depending on your setup:\n",
    "\n",
    "1. If you're already using a [Neptune Analytics notebook](https://docs.aws.amazon.com/neptune-analytics/latest/userguide/notebooks.html), that has access to the graph you created, you can skip this section\n",
    "2. If you're using a regular SageMaker notebook instance, you can install the graph-notebook extension:\n",
    "   ```bash\n",
    "   pip install graph-notebook\n",
    "   jupyter nbextension enable --py --sys-prefix graph_notebook_widgets\n",
    "   ```\n",
    "3. If you're self-hosting Jupyter, you can follow the instructions in \n",
    "   [Hosting a Neptune Analytics graph-notebook on your local machine](https://docs.aws.amazon.com/neptune-analytics/latest/userguide/create-notebook-local.html)\n",
    "\n",
    "If you choose to create a new Neptune Analytics notebook, follow these steps.\n",
    "\n",
    "### Required IAM Role\n",
    "For a demo role with wider permissions you can attach the following policies to the notebook instance role:\n",
    "- AWSNeptuneAnalyticsFullAccess\n",
    "- AmazonSageMakerFullAccess\n",
    "\n",
    "For detailed permissions and trust policy requirements, see the [Neptune Analytics documentation](https://docs.aws.amazon.com/neptune-analytics/latest/userguide/create-notebook-console.html#create-notebook-iam-role)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import base64\n",
    "\n",
    "sm_client = boto3.client(\"sagemaker\", region_name=AWS_REGION)\n",
    "\n",
    "# Create a lifecycle config for a Neptune Analytics graph notebook\n",
    "\n",
    "# TODO: Add a download of the 5th notebook to this LCC script, once the code is public\n",
    "start_script = r\"\"\"#!/bin/bash\n",
    "\n",
    "sudo -u ec2-user -i <<'EOF'\n",
    "\n",
    "echo \"export GRAPH_NOTEBOOK_AUTH_MODE=IAM\" >> ~/.bashrc\n",
    "echo \"export GRAPH_NOTEBOOK_SSL=True\" >> ~/.bashrc\n",
    "echo \"export GRAPH_NOTEBOOK_SERVICE=neptune-graph\" >> ~/.bashrc\n",
    "echo \"export GRAPH_NOTEBOOK_HOST=GRAPH_ID_PLACEHOLDER.REGION_PLACEHOLDER.neptune-graph.amazonaws.com\" >> ~/.bashrc\n",
    "echo \"export GRAPH_NOTEBOOK_PORT=8182\" >> ~/.bashrc\n",
    "echo \"export NEPTUNE_LOAD_FROM_S3_ROLE_ARN=\" >> ~/.bashrc\n",
    "echo \"export AWS_REGION=REGION_PLACEHOLDER\" >> ~/.bashrc\n",
    "\n",
    "aws s3 cp s3://aws-neptune-notebook-REGION_PLACEHOLDER/graph_notebook.tar.gz /tmp/graph_notebook.tar.gz\n",
    "rm -rf /tmp/graph_notebook\n",
    "tar -zxvf /tmp/graph_notebook.tar.gz -C /tmp\n",
    "chmod +x /tmp/graph_notebook/install_jl4x.sh\n",
    "/tmp/graph_notebook/install_jl4x.sh\n",
    "\n",
    "EOF\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "start_script = (start_script\n",
    "    .replace(\"GRAPH_ID_PLACEHOLDER\", GRAPH_ID)\n",
    "    .replace(\"REGION_PLACEHOLDER\", AWS_REGION)\n",
    "    )\n",
    "\n",
    "# Encode string to bytes using utf-8, encode bytes to base64, then decode base64 to string\n",
    "encoded_script = base64.b64encode(start_script.encode()).decode()\n",
    "lc_config_name = f\"{GRAPH_NAME}-{GRAPH_ID}-LC\"\n",
    "\n",
    "response = sm_client.create_notebook_instance_lifecycle_config(\n",
    "    NotebookInstanceLifecycleConfigName=lc_config_name,\n",
    "    OnCreate=[\n",
    "        {\"Content\": encoded_script},\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the lifecycle config available next you will launch the actual notebook instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your Neptune analytics notebook role here\n",
    "NEPTUNE_NOTEBOOK_ROLE = \"arn:aws:iam::123456789012:role/<Your-NeptuneAnalytics-Notebook-Role>\"\n",
    "\n",
    "response = sm_client.create_notebook_instance(\n",
    "    NotebookInstanceName=f\"{GRAPH_NAME}-{GRAPH_ID}-notebook\",\n",
    "    InstanceType=\"ml.t3.medium\",\n",
    "    RoleArn=NEPTUNE_NOTEBOOK_ROLE,\n",
    "    LifecycleConfigName=lc_config_name,\n",
    "    DirectInternetAccess=\"Enabled\",\n",
    "    VolumeSizeInGB=50,\n",
    "    PlatformIdentifier=\"notebook-al2-v3\", # Ensure we create Jupyterlab v4 notebook\n",
    "    InstanceMetadataServiceConfiguration={\"MinimumInstanceMetadataServiceVersion\": \"2\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the notebook instance is being set up, move on to the next notebook, `2-Deploy-Execute-Pipeline.ipynb` to create a SageMaker training pipeline "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
