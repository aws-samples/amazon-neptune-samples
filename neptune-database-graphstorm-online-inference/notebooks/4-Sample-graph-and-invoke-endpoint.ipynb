{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3df94c00",
   "metadata": {},
   "source": [
    "# How to sample a graph neighborhood from Neptune Database and invoke the GraphStorm SageMaker endpoint\n",
    "\n",
    "Now that that you have loaded the graph data into Neptune and deployed the SageMaker endpoint into the same VPC,\n",
    "you can use the SageMaker Notebook instance as the intermediary between the two to gather a graph sample, convert it\n",
    "to the a JSON payload formatted as GraphStorm expects and receive a prediction for it.\n",
    "\n",
    "In this notebook we illustrate how to retrieve a k-hop graph neighborhood sample from Neptune Database using the Gremlin\n",
    "graph query language, convert the JSON response into the format GraphStorm inference expects, and post a prediction request\n",
    "to the SageMaker endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99407d1",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "**This notebook is designed to run from within the SageMaker Notebook instance that you created as part of the CDK deployment.**\n",
    "\n",
    "This is necessary, as we set up the VPC and security groups to allow the SageMaker Notebook to post queries to the Neptune Database instance the CDK created.\n",
    "\n",
    "**You also need to ensure you are using the `Python 3` kernel for this notebook**, as\n",
    "that provides access to the graph notebook magics like `%graph_notebook_config` that we\n",
    "use to retrieve information about the Neptune Database instance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedee7fe",
   "metadata": {},
   "source": [
    "## Environment setup\n",
    "\n",
    "To start set up your environment with the necessary imports and configuration.\n",
    "\n",
    "We need to know which Neptune DB host we will use during graph sampling, which we get from the graph notebook's configuration, and the name of the SageMaker endpoint we will send inference requests to, which you will need to provide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827bbfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import yaml\n",
    "\n",
    "import boto3\n",
    "import requests\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util import Retry\n",
    "import urllib3\n",
    "from botocore.auth import SigV4Auth\n",
    "from botocore.awsrequest import AWSRequest\n",
    "\n",
    "# Configure retry strategy\n",
    "retry_strategy = Retry(total=3, backoff_factor=1, status_forcelist=[500, 502, 503, 504])\n",
    "\n",
    "# Set up AWS credentials for request signing\n",
    "session = boto3.Session()\n",
    "credentials = session.get_credentials()\n",
    "region = session.region_name\n",
    "\n",
    "# Set up session with retry\n",
    "urllib3.disable_warnings()\n",
    "http_session = requests.Session()\n",
    "adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "http_session.mount(\"https://\", adapter)\n",
    "http_session.verify = False\n",
    "\n",
    "# Neptune endpoint configuration\n",
    "config_obj = %graph_notebook_config\n",
    "NEPTUNE_HOST = config_obj.host\n",
    "GREMLIN_PORT = 8182"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6919bb-ba0e-441c-90c1-260a27f8560b",
   "metadata": {},
   "source": [
    "## ENTER ENDPOINT NAME\n",
    "\n",
    "In notebook `2-GraphStorm-Endpoint-Deployment.ipynb` you deployed a SageMaker realtime endpoint. During deployment the script printed the endpoint name which should be of the form `ieee-fraud-detection-Endpoint-<timestamp>`. Provide that name here so we can use it to post requests.\n",
    "\n",
    "The endpoint name can also be found from [Amazon SageMaker AI Web console](https://us-east-1.console.aws.amazon.com/sagemaker/home?region=us-east-1#/endpoints) under the``Inference -> Endpoints`` menu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c9d633-fa28-43e8-a816-89f44caef688",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENDPOINT_NAME = \"<YOUR-ENDPOINT-NAME-HERE>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8dea408",
   "metadata": {},
   "source": [
    "### Executing Gremlin queries\n",
    "\n",
    "Next, we will set up a Python function that allows us to send authenticated Gremlin queries to the Neptune Database. It is necessary to sign your requests because the  we chose IAM authentication when creating the NeptuneDB instance in the CDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d92c763",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_gremlin_query(query):\n",
    "    \"\"\"Execute a Gremlin query using HTTPS POST request.\n",
    "\n",
    "    Args:\n",
    "        query: Gremlin query string\n",
    "\n",
    "    Returns:\n",
    "        dict: Query response\n",
    "    \"\"\"\n",
    "    endpoint_url = f\"https://{NEPTUNE_HOST}:{GREMLIN_PORT}/gremlin\"\n",
    "\n",
    "    # Create request for signing\n",
    "    request = AWSRequest(\n",
    "        method=\"POST\",\n",
    "        url=endpoint_url,\n",
    "        headers={\"Host\": NEPTUNE_HOST},\n",
    "        data=json.dumps({\"gremlin\": query}),\n",
    "    )\n",
    "    SigV4Auth(credentials, \"neptune-db\", region).add_auth(request)\n",
    "\n",
    "    # Execute request\n",
    "    headers = dict(request.headers)\n",
    "    headers[\"Host\"] = NEPTUNE_HOST\n",
    "    headers[\"Content-Type\"] = \"application/json\"\n",
    "\n",
    "    response = http_session.post(endpoint_url, headers=headers, json={\"gremlin\": query})\n",
    "    response.raise_for_status()\n",
    "\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130e5e1c",
   "metadata": {},
   "source": [
    "We can use this function to post simple queries to the Neptune Database, for example retrieve the ids of 100 `Transaction` nodes. We can use these node ids later to measure inference latency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95ae810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get test transaction IDs\n",
    "query = \"g.V().hasLabel('Transaction').limit(100).id()\"\n",
    "response = execute_gremlin_query(query)\n",
    "test_tx_ids = response[\"result\"][\"data\"][\"@value\"]\n",
    "\n",
    "print(f\"Found {len(test_tx_ids)} transactions ids for testing: {test_tx_ids[:5]} ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d998fd9c",
   "metadata": {},
   "source": [
    "## Graph Sampling in Neptune DB\n",
    "\n",
    "Next we define a function that performs the graph neighborhood sampling using Gremlin.\n",
    "\n",
    "This function will gradually build a Gremlin query string based on the sampling criteria we provide as arguments.\n",
    "\n",
    "Remember that GNN sampling for node classification happens from the outside in: After selecting the node you want to predict for, called the \"target node\", we take a sample of nodes that point to it, its \"incoming neighbors\". Then for each of these selected neighbors we repeat the process, up to a maximum number of hops `k`, which is determined by the number of layers used when training your GNN.\n",
    "\n",
    "To make the process scalable we limit the number of maximum neighbors to retrieve for each node by a value called the \"fanout\".\n",
    "\n",
    "For example a 2-hop, 10,10 fanout indicates that we will try to sample 10 nodes at the first level (direct neighbors of the target), then for every sampled node we will again try to sample another nodes. \n",
    "\n",
    "To retrieve the edges as well as the nodes from Neptune DB we will post two separate queries, one to get the nodes and one to get the edges between them.\n",
    "\n",
    "The `sample_graph` function will return two Python dicts, the first being the response containing all nodes in the sample, and the second being the edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55eea85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_graph(tx_id, hop_count, fanout, add_reverse=True):\n",
    "    \"\"\"Sample local neighborhood of a transaction from Neptune instance\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        tx_id:\n",
    "            Target node ID\n",
    "        hop_count:\n",
    "            Number of hops\n",
    "        fanout:\n",
    "            Max of neighbors to visit per node at each step\n",
    "        add_reverse:\n",
    "            Whether to consider reverse edges during traversal.\n",
    "            When true, edge directionality is ignored during sampling,\n",
    "            otherwise we use inE() to follow incoming edges, starting\n",
    "            from the target outwards.\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        tuple[dict, dict]: (node_response, edges_response)\n",
    "            Tuple with Gremlin responses for nodes in the first element\n",
    "            and edges response in the second element.\n",
    "    \"\"\"\n",
    "    # Query 1: Get nodes\n",
    "    node_query = f\"g.V('{tx_id}').store('subGraph').\"\n",
    "    if add_reverse:\n",
    "        # Ignore edge directionality\n",
    "        node_query += f\"repeat(bothE().limit({fanout}).otherV().store('subGraph')).\"\n",
    "    else:\n",
    "        # Only get incoming neighbors\n",
    "        node_query += f\"repeat(inE().limit({fanout}).otherV().store('subGraph')).\"\n",
    "    node_query += f\"times({hop_count}).\"\n",
    "    node_query += \"cap('subGraph').unfold().dedup().\"\n",
    "    node_query += \"project('nodeId', 'nodeType', 'properties').\"\n",
    "    node_query += \"by(id()).by(label()).by(valueMap())\"\n",
    "\n",
    "    # Query 2: Get edges between sampled nodes\n",
    "    # TODO: Support edge property extraction\n",
    "    edge_query = f\"g.V('{tx_id}').store('subGraph').\"\n",
    "    if add_reverse:\n",
    "        edge_query += f\"repeat(bothE().limit({fanout}).store('edges').otherV().store('subGraph')).\"\n",
    "    else:\n",
    "        edge_query += (\n",
    "            f\"repeat(inE().limit({fanout}).store('edges').otherV().store('subGraph')).\"\n",
    "        )\n",
    "    edge_query += f\"times({hop_count}).\"\n",
    "    edge_query += \"cap('edges').unfold().dedup().\"\n",
    "    edge_query += (\n",
    "        \"project('edgeId', 'edgeType', 'fromId', 'fromType', 'toId', 'toType').\"\n",
    "    )\n",
    "    edge_query += \"by(id()).by(label()).by(outV().id()).by(outV().label()).by(inV().id()).by(inV().label())\"\n",
    "\n",
    "    # Execute queries\n",
    "    node_response = execute_gremlin_query(node_query)\n",
    "    edge_response = execute_gremlin_query(edge_query)\n",
    "\n",
    "    return node_response, edge_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996026e8",
   "metadata": {},
   "source": [
    "Now that you have a way to sample the graph, try retrieving a sample for a single Transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1caa4fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_id = test_tx_ids[0]\n",
    "\n",
    "node_response, edge_response = sample_graph(\n",
    "    tx_id, hop_count=2, fanout=10, add_reverse=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fee5d3",
   "metadata": {},
   "source": [
    "The `node_response` and `edge_response` contain responses in the [GraphSON](https://tinkerpop.apache.org/docs/3.7.4/dev/io/#graphson) specification, Gremlin's JSON specification that serializes graph objects.\n",
    "\n",
    "For example, to see the data retrieved for the first node in the node response you can use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d902d3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_response[\"result\"][\"data\"][\"@value\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38424190",
   "metadata": {},
   "source": [
    "Similarly, you can view the data for the first edge using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8a2a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_response[\"result\"][\"data\"][\"@value\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd8f6ea",
   "metadata": {},
   "source": [
    "## Converting Neptune response to GraphStorm format\n",
    "\n",
    "[GraphStorm uses its own JSON specification](https://graphstorm.readthedocs.io/en/latest/cli/model-training-inference/real-time-inference-spec.html) for inference requests that's designed with GNN inference in mind. To make it possible to use the data retrieved from Neptune to make a prediction using the GraphStorm endpoint, you will need to convert the retrieved graph from the GraphSON format to the format that GraphStorm expects. \n",
    "\n",
    "To ease this process we provide the conversion code for the response of the specific Gremlin query define in `sample_graph` in `convert_neptune_gs_sample.py`.\n",
    "\n",
    "The function of interest is `prepare_payload` which takes as input the nodes and edges responses from the Gremlin query, uses the GraphStorm graph construction and training configurations to validate and transform data to the appropriate formats, and returns a dictionary that matches the GraphStorm real-time inference specification and can be used directly to make inference requests to the endpoint. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be05d92",
   "metadata": {},
   "source": [
    "First, you'll need to retrieve the graph construction and training configuration files, to ensure the requests you make to the endpoint match the expected schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675a6100",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "with open(\"task_config.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    task_config = json.load(f)\n",
    "\n",
    "MODEL_PATH = task_config[\"MODEL_PATH\"]\n",
    "\n",
    "with open(\n",
    "    os.path.join(MODEL_PATH, \"GRAPHSTORM_RUNTIME_UPDATED_TRAINING_CONFIG.yaml\"),\n",
    "    \"r\",\n",
    "    encoding=\"utf-8\",\n",
    ") as f:\n",
    "    train_config_dict = yaml.safe_load(f)\n",
    "\n",
    "with open(\n",
    "    os.path.join(MODEL_PATH, \"data_transform_new.json\"),\n",
    "    \"r\",\n",
    "    encoding=\"utf-8\",\n",
    ") as f:\n",
    "    gconstruct_config_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a2e739",
   "metadata": {},
   "source": [
    "Now you can try converting the payload of the test transaction from the GraphSON format to the GraphStorm inference format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58e8d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from convert_neptune_gs_sample import prepare_payload\n",
    "\n",
    "test_payload = prepare_payload(\n",
    "    node_response,\n",
    "    edge_response,\n",
    "    tx_id,\n",
    "    gconstruct_config_dict,\n",
    "    train_config_dict,\n",
    "    target_node_type=\"Transaction\",\n",
    "    add_reverse=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce2cc54",
   "metadata": {},
   "source": [
    "As before you can try inspecting the first node and first edge in the payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed34864c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_payload[\"graph\"][\"nodes\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfe6581",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_payload[\"graph\"][\"edges\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13932dd8",
   "metadata": {},
   "source": [
    "## Making an inference request using the GraphStorm payload\n",
    "\n",
    "Now that you have prepared the data in the format that GraphStorm expects, you are able to make a prediction for the target node using the SageMaker endpoint!\n",
    "\n",
    "We will write a small function that allows us to invoke the endpoint using `boto3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b996cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_endpoint(payload):\n",
    "    \"\"\"Invoke SageMaker endpoint for inference.\n",
    "\n",
    "    Args:\n",
    "        payload: GraphStorm inference request payload\n",
    "\n",
    "    Returns:\n",
    "        dict: Inference response\n",
    "    \"\"\"\n",
    "    sagemaker_runtime = boto3.client(\"sagemaker-runtime\")\n",
    "    response = sagemaker_runtime.invoke_endpoint(\n",
    "        EndpointName=ENDPOINT_NAME,\n",
    "        ContentType=\"application/json\",\n",
    "        Accept=\"application/json\",\n",
    "        Body=json.dumps(payload),\n",
    "    )\n",
    "\n",
    "    result = json.loads(response[\"Body\"].read().decode())\n",
    "\n",
    "    # Check response status code\n",
    "    if result.get(\"status_code\", 200) != 200:\n",
    "        raise Exception(f\"Inference failed: {result.get('error', 'Unknown error')}\")\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce07a4d",
   "metadata": {},
   "source": [
    "Note that in order to be able to make SageMaker runtime invocations through from the VPC, we have created a VPC endpoint for SageMaker runtime during the CDK deployment. This is necessary to be able to send the requests from the Notebook instance that lies within the VPC.\n",
    "\n",
    "Try making a prediction with the test payload you just created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73034019",
   "metadata": {},
   "outputs": [],
   "source": [
    "invoke_endpoint(test_payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f661c033",
   "metadata": {},
   "source": [
    "You should be able to see a response in the format (results will vary depending on which transaction ID was used)\n",
    "\n",
    "```json\n",
    "{'status_code': 200,\n",
    " 'request_uid': '877042dbc361fc33',\n",
    " 'message': 'Request processed successfully.',\n",
    " 'error': '',\n",
    " 'data': {\n",
    "    'results': [\n",
    "            {\n",
    "                'node_type': 'Transaction',\n",
    "                'node_id': '2991260',\n",
    "                'prediction': [0.995966911315918, 0.004033133387565613]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "The data of interest for the single transaction we made a prediction for are in the `prediction` key and corresponding `node_id`. The prediction gives us the raw scores the model produces for class 0 (legitimate) and class 1 (fraudulent), at the corresponding 0 and 1 indexes of the `predictions` list. In this example, the model marks the transaction as most likely legitimate.\n",
    "\n",
    "You can find the full GraphStorm response specification in the [GraphStorm documentation](https://graphstorm.readthedocs.io/en/latest/cli/model-training-inference/real-time-inference-spec.html#specification-of-response-body-contents)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f5ee58",
   "metadata": {},
   "source": [
    "## Latency measurements\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583e4269",
   "metadata": {},
   "source": [
    "Now that you learned how perform a single inference call you can go ahead and perform multiple end-to-end inference requests to measure response latency.\n",
    "\n",
    "As demonstrated, an end-to-end inference call for a node that already exists in the graph will have 3 distinct stages:\n",
    "\n",
    "1. Graph sampling from Neptune DB.\n",
    "    * For a given target node that already exists in the graph, retrieve its k-hop neighborhood with a fanout limit, i.e. limiting the number of neighbors retrieved at each hop by a threshold.\n",
    "2. Payload preparation for inference\n",
    "    * Neptune DB returns graphs using [GraphSON](https://tinkerpop.apache.org/docs/3.7.4/dev/io/#graphson), a specialized JSON-like data format used to describe graph data. At this step we need to convert the returned GraphSON to GraphStormâ€™s ownJSON spec. This step is performed on the inference client, in this case a SageMaker notebook instance.\n",
    "3. Model inference via SageMaker endpoint\n",
    "    * Once the payload is prepared, we send an inference request to a SageMaker endpoint that has loaded a previously trained model snapshot. The endpoint receives the request, performs any feature transformations needed (e.g. converting categorical features to one-hot encoding), creates the binary graph representation in memory and makes a prediction for the target node using the graph neighborhood and trained model weights. The response is encoded to JSON and sent back to the client.\n",
    "\n",
    "\n",
    "To conclude this investigation you will perform a series of inference calls for a random set of target nodes, measuring the latency of each stage and the total end-to-end latency. This should give you an indicate of what sort of latencies to expect for within-VPC inference.\n",
    "\n",
    "First, define a function that performs end-to-end inference and measures the latency of each stage for a single inference call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5d5539",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_e2e_latency(\n",
    "    tx_id,\n",
    "    hop_count,\n",
    "    fanout,\n",
    "    gconstruct_config_dict,\n",
    "    train_config_dict,\n",
    "    target_node_type=\"Transaction\",\n",
    "    add_reverse=True,\n",
    "):\n",
    "    \"\"\"Measure end-to-end latency for a single transaction.\n",
    "\n",
    "    Args:\n",
    "        tx_id: Transaction ID\n",
    "        hop_count: Number of hops\n",
    "        fanout: Number of edges per node\n",
    "        add_reverse: Whether to add reverse edges in the payload\n",
    "\n",
    "    Returns:\n",
    "        dict: Latency measurements\n",
    "    \"\"\"\n",
    "    # 1. Graph sampling\n",
    "    sampling_start = time.time()\n",
    "    node_response, edge_response = sample_graph(tx_id, hop_count, fanout, add_reverse)\n",
    "    sampling_end = time.time()\n",
    "\n",
    "    # 2. Payload preparation\n",
    "    prep_start = time.time()\n",
    "    payload = prepare_payload(\n",
    "        node_response,\n",
    "        edge_response,\n",
    "        tx_id,\n",
    "        gconstruct_config_dict,\n",
    "        train_config_dict,\n",
    "        target_node_type=target_node_type,\n",
    "        add_reverse=add_reverse,\n",
    "    )\n",
    "    prep_end = time.time()\n",
    "\n",
    "    # 3. Inference request\n",
    "    inference_start = time.time()\n",
    "    _ = invoke_endpoint(payload)\n",
    "    inference_end = time.time()\n",
    "\n",
    "    return {\n",
    "        \"sampling_latency\": (sampling_end - sampling_start) * 1000,\n",
    "        \"prep_latency\": (prep_end - prep_start) * 1000,\n",
    "        \"inference_latency\": (inference_end - inference_start) * 1000,\n",
    "        \"total_latency\": (inference_end - sampling_start) * 1000,\n",
    "        \"num_nodes\": len(payload[\"graph\"][\"nodes\"]),\n",
    "        \"num_edges\": len(payload[\"graph\"][\"edges\"]),\n",
    "        \"payload_size\": len(json.dumps(payload)),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94cd9fc",
   "metadata": {},
   "source": [
    "With this function you can measure the latency of a single end-to-end inference call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751b1fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with first transaction\n",
    "tx_id = test_tx_ids[0]\n",
    "result = measure_e2e_latency(\n",
    "    tx_id,\n",
    "    hop_count=2,\n",
    "    fanout=10,\n",
    "    gconstruct_config_dict=gconstruct_config_dict,\n",
    "    train_config_dict=train_config_dict,\n",
    ")\n",
    "\n",
    "print(\"Latency breakdown:\")\n",
    "print(f\"- Graph sampling: {result['sampling_latency']:.2f} ms\")\n",
    "print(f\"- Payload preparation: {result['prep_latency']:.2f} ms\")\n",
    "print(f\"- Inference: {result['inference_latency']:.2f} ms\")\n",
    "print(f\"- Total: {result['total_latency']:.2f} ms\")\n",
    "\n",
    "print(\"\\nGraph size:\")\n",
    "print(f\"- Nodes: {result['num_nodes']}\")\n",
    "print(f\"- Edges: {result['num_edges']}\")\n",
    "print(f\"- Payload size: {result['payload_size']} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8006c81",
   "metadata": {},
   "source": [
    "You'll see we are capturing various aspects in the statistics, the latency of each stage, as well as the size of the graph in terms of number of nodes, edges, and payload byte size.\n",
    "\n",
    "Using the `measure_e2e_latency` function, you can now define a comprehensive experiment that will perform multiple calls and collect statistics as a Pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472e42d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(\n",
    "    tx_ids,\n",
    "    gconstruct_config_dict,\n",
    "    train_config_dict,\n",
    "    hop_counts,\n",
    "    fanouts,\n",
    "    num_trials,\n",
    "    add_reverse=True,\n",
    "):\n",
    "    \"\"\"Run latency measurement experiment.\n",
    "\n",
    "    Args:\n",
    "        tx_ids: List of transaction IDs\n",
    "        hop_counts: List of hop counts to test\n",
    "        fanouts: List of fanout values to test\n",
    "        num_trials: Number of trials per configuration\n",
    "        add_reverse: Whether to add reverse edges in the payload\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Experiment results\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for hop in hop_counts:\n",
    "        for fanout in fanouts:\n",
    "            print(f\"Testing hop_count={hop}, fanout={fanout}\")\n",
    "            for tx_id in tx_ids[:num_trials]:\n",
    "                try:\n",
    "                    result = measure_e2e_latency(\n",
    "                        tx_id,\n",
    "                        hop,\n",
    "                        fanout,\n",
    "                        gconstruct_config_dict=gconstruct_config_dict,\n",
    "                        train_config_dict=train_config_dict,\n",
    "                        add_reverse=add_reverse,\n",
    "                    )\n",
    "                    results.append(\n",
    "                        {\n",
    "                            \"tx_id\": tx_id,\n",
    "                            \"hop_count\": hop,\n",
    "                            \"fanout\": fanout,\n",
    "                            \"sampling_latency\": result[\"sampling_latency\"],\n",
    "                            \"prep_latency\": result[\"prep_latency\"],\n",
    "                            \"inference_latency\": result[\"inference_latency\"],\n",
    "                            \"total_latency\": result[\"total_latency\"],\n",
    "                            \"num_nodes\": result[\"num_nodes\"],\n",
    "                            \"num_edges\": result[\"num_edges\"],\n",
    "                            \"payload_size\": result[\"payload_size\"],\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "                    # Small sleep to avoid overwhelming the service\n",
    "                    time.sleep(0.1)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {tx_id}: {e}\")\n",
    "                    continue\n",
    "\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cff001",
   "metadata": {},
   "source": [
    "Since you already sampled 100 transaction IDs you can use those to run the full experiment.\n",
    "\n",
    "The `run_experiment` function can test various configurations for number of hops and different fanouts to test.\n",
    "\n",
    "The model we're testing against was trained with a 2-hop, 10,10 fanout so we'll test that configuration, and a couple more\n",
    "to measure scaling characteristics, namely 1-hop sampling and 100,100 fanout for both k-hop configs. \n",
    "\n",
    "With the given configuration, the experiment should take 3-4 minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fafda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment configuration\n",
    "experiment_config = {\"hop_counts\": [1, 2], \"fanouts\": [10, 100], \"num_trials\": 100}\n",
    "\n",
    "# Run experiment\n",
    "print(\"Running experiment...\")\n",
    "results_df = run_experiment(\n",
    "    test_tx_ids,\n",
    "    gconstruct_config_dict,\n",
    "    train_config_dict,\n",
    "    experiment_config[\"hop_counts\"],\n",
    "    experiment_config[\"fanouts\"],\n",
    "    experiment_config[\"num_trials\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f5c3e9",
   "metadata": {},
   "source": [
    "The `results_df` pandas Dataframe holds the metrics for each individual inference call, so you can aggregate the statistics from it to get an overall idea of latency characteristics.\n",
    "\n",
    "To begin, take a look at the total latency statistics, which measures the end-to-end latency of each inference call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb78d9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate statistics per configuration\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "stats = (\n",
    "    results_df.groupby([\"hop_count\", \"fanout\"])\n",
    "    .agg(\n",
    "        {\n",
    "            \"total_latency\": [\n",
    "                \"mean\",\n",
    "                \"median\",\n",
    "                \"std\",\n",
    "                \"min\",\n",
    "                \"max\",\n",
    "                (\"p90\", lambda x: np.percentile(x, 90)),\n",
    "                (\"p99\", lambda x: np.percentile(x, 99)),\n",
    "            ],\n",
    "        }\n",
    "    )\n",
    "    .round(2)\n",
    ")\n",
    "\n",
    "print(\"Latency Statistics (ms):\")\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8b4368",
   "metadata": {},
   "source": [
    "**NOTE**: *The reference numbers we provide were tested on a ml.m5.4xlarge notebook instance, an ml.c6i.xlarge endpoint, and r8g.4xlarge NeptuneDB cluster. Latency measurements will vary depending on factors like network latency.*\n",
    "\n",
    "With the default configuration, you will see that the median latency for a 2-hop, 10,10 inference call will be around 140ms, while p99 is ~290ms, meaning that 99/100 inference calls took less than 290ms to complete end-to-end.\n",
    "\n",
    "We can dig more into each aspect of the inference call to gain more insights into the latency of each component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6028672a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot latency distributions\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Sampling latency\n",
    "plt.subplot(131)\n",
    "sns.boxplot(data=results_df, x=\"fanout\", y=\"sampling_latency\", hue=\"hop_count\")\n",
    "plt.title(\"Graph Sampling Latency\")\n",
    "plt.xlabel(\"Fanout\")\n",
    "plt.ylabel(\"Latency (ms)\")\n",
    "\n",
    "# Payload preparation latency\n",
    "plt.subplot(132)\n",
    "sns.boxplot(data=results_df, x=\"fanout\", y=\"prep_latency\", hue=\"hop_count\")\n",
    "plt.title(\"Payload Preparation Latency\")\n",
    "plt.xlabel(\"Fanout\")\n",
    "plt.ylabel(\"Latency (ms)\")\n",
    "\n",
    "# Model inference latency\n",
    "plt.subplot(133)\n",
    "sns.boxplot(data=results_df, x=\"fanout\", y=\"inference_latency\", hue=\"hop_count\")\n",
    "plt.title(\"Model Inference Latency\")\n",
    "plt.xlabel(\"Fanout\")\n",
    "plt.ylabel(\"Latency (ms)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a42c5ec",
   "metadata": {},
   "source": [
    "In the figure above you can focus on the `hop_count` 2 entries, the right box plot for each `Fanout` configuration on the X-axis (colors can be hard to make out for tight box plots).\n",
    "\n",
    "You'll see that for hop count == 2, fanout == \"10,10\", both the graph sampling latency and payload preparation latencies are tight distributions ~50ms, while model inference can be a bit more varied, with most calls around 70-100ms.\n",
    "\n",
    "One interesting observation is that when scaling up the fanout to \"100,100\", model inference latency increases sub-linearly. We can look into how graph size affects each aspect of inference with another figure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc737fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot size vs latency relationships\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Node count vs sampling latency\n",
    "plt.subplot(131)\n",
    "sns.scatterplot(\n",
    "    data=results_df,\n",
    "    x=\"num_nodes\",\n",
    "    y=\"sampling_latency\",\n",
    "    hue=\"hop_count\",\n",
    "    style=\"fanout\",\n",
    ")\n",
    "plt.title(\"Node Count vs Sampling Latency\")\n",
    "plt.xlabel(\"Number of Nodes\")\n",
    "plt.ylabel(\"Latency (ms)\")\n",
    "\n",
    "\n",
    "# Graph size vs payload preparation\n",
    "plt.subplot(132)\n",
    "sns.scatterplot(\n",
    "    data=results_df, x=\"num_nodes\", y=\"prep_latency\", hue=\"hop_count\", style=\"fanout\"\n",
    ")\n",
    "plt.title(\"Node Count vs Payload Preparation Latency\")\n",
    "plt.xlabel(\"Number of Nodes\")\n",
    "plt.ylabel(\"Latency (ms)\")\n",
    "\n",
    "# Graph size vs inference latency\n",
    "plt.subplot(133)\n",
    "sns.scatterplot(\n",
    "    data=results_df,\n",
    "    x=\"num_nodes\",\n",
    "    y=\"inference_latency\",\n",
    "    hue=\"hop_count\",\n",
    "    style=\"fanout\",\n",
    ")\n",
    "plt.title(\"Node Count vs Model Inference Latency\")\n",
    "plt.xlabel(\"Number of Nodes\")\n",
    "plt.ylabel(\"Latency (ms)\")\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d81a3a",
   "metadata": {},
   "source": [
    "From the figure above you can see that sampling latency is generally correlated to the number of nodes being retrieved, while the payload preparation is perfectly correlated to node count, which is expected since we sequentially process every node in the GraphSON response. This tells us that there is definite optimization opportunities there, should we wish to reduce end-to-end latency.\n",
    "\n",
    "Finally we can see that model inference scales sub-linearly with node count: making a prediction for a 1000 node subgraph can take as little as one for 100 nodes, which is the result of the efficient parallel implementation backing GraphStorm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1638eac",
   "metadata": {},
   "source": [
    "To conclude let's take a look at the mean latency broken down by component and number of hops/fanout:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c7ab64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create heatmaps for each component\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "components = [\"sampling_latency\", \"prep_latency\", \"inference_latency\"]\n",
    "titles = [\"Sampling\", \"Preparation\", \"Inference\"]\n",
    "\n",
    "pivot_total = results_df.pivot_table(\n",
    "    values=[\"sampling_latency\", \"prep_latency\", \"inference_latency\"],\n",
    "    index=\"hop_count\",\n",
    "    columns=\"fanout\",\n",
    "    aggfunc=\"mean\",\n",
    ")\n",
    "\n",
    "for ax, component, title in zip(axes, components, titles):\n",
    "    sns.heatmap(pivot_total[component], ax=ax, annot=True, fmt=\".1f\", cmap=\"YlOrRd\")\n",
    "    ax.set_title(f\"{title} Latency by Hop/Fanout\")\n",
    "    ax.set_xlabel(\"Fanout\")\n",
    "    ax.set_ylabel(\"Number of Hops\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bddd9a",
   "metadata": {},
   "source": [
    "This is another view into the per-component latency. You can see that while for smaller graphs (2-hop, \"10,10\" fanout) the model inference latency (~90ms) is roughly equal to the combined latency for sampling and preparation, for larger graphs, the sampling+preparation stages dominate the inference time (\\~600ms sampling+preparation vs. \\~130ms model inference). \n",
    "\n",
    "Thankfully sampling and preparation latency can be reduced by more efficient Gremlin queries and payload preparation implementation, which can be a topic for another blog post."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0535a465",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook we demonstrated how you can perform end-to-end online inference of graph neural networks, using a GraphStorm-trained model deployed as a SageMaker Inference Endpoint, using Neptune Database as the source of your graph data. In summary you learned the following:\n",
    "\n",
    "1. How to use the Gremlin query language perform k-hop ego-network sampling that is consistent with how GNN models are trained.\n",
    "2. How the responses coming from a Gremlin query look like, and how to transform those to the specification that GraphStorm expects for online inference.\n",
    "3. How to use boto3 to send an inference request to a SageMaker real-time endpoint.\n",
    "4. How to measure latency for end-to-end GNN inference calls.\n",
    "\n",
    "GraphStorm and Neptune provide a unique combination that allows you to perform GNN inference online while maintaining full control of your model and graph source in-house.\n",
    "\n",
    "### Next steps\n",
    "\n",
    "In this solution we demonstrated how to post inference calls from a client (in this case a SageMaker Notebook instance) that resides inside the same VPC as the Neptune DB cluster. \n",
    "\n",
    "If your inference calls will come from external entities, e.g. a customer hitting a REST API on a different VPC, you will need to determine what's the best way for the inference client to access the Neptune cluster, e.g. using a [VPC peering connection](https://docs.aws.amazon.com/neptune/latest/userguide/get-started-connect-ec2-other-vpc.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a124bf3",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "To clean up all the resources created for this project you will need to:\n",
    "\n",
    "#### 1. Delete the SageMaker endpoint once you are done testing\n",
    "\n",
    "First, to delete the endpoint you can run\n",
    "\n",
    "```python\n",
    "import boto3\n",
    "\n",
    "# Create a low-level SageMaker service client.\n",
    "sagemaker_client = boto3.client('sagemaker', region_name=config_obj.aws_region)\n",
    "# Delete endpoint\n",
    "sagemaker_client.delete_endpoint(EndpointName=ENDPOINT_NAME)\n",
    "```\n",
    "\n",
    "See the [SageMaker documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints-delete-resources.html) on more options to delete an endpoint.\n",
    "\n",
    "It's important to delete the endpoint **before** running `cdk destroy`, otherwise\n",
    "`cdk destroy` might fail. The reason is that the endpoint is using a network interface\n",
    "within the VPC, so we can't destroy the VPC before all network interfaces attached\n",
    "to it are deleted.\n",
    "\n",
    "#### 2. Destroy the CloudFormation stack deployed through CDK.\n",
    "\n",
    "\n",
    "**After deleting all deployed endpoints**, to destroy the CDK project you can run\n",
    "\n",
    "```bash\n",
    "# Navigate to the CDK project where you initially deployed it\n",
    "cd neptune-db-cdk\n",
    "# Make sure to provide any context variables if used\n",
    "cdk destroy # [--context prefix=dev-]\n",
    "```\n",
    "\n",
    "See the [CDK docs](https://docs.aws.amazon.com/cdk/v2/guide/ref-cli-cmd-destroy.html) on how to use `cdk destroy`, or the CloudFormation docs on how to [delete a stack from the console UI](https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-console-delete-stack.html).\n",
    "\n",
    "#### 3. [Optional] Empty and destroy the S3 bucket that holds the data model artifacts\n",
    "\n",
    "\n",
    "Finally, you can [follow the AWS documentation](https://docs.aws.amazon.com/AmazonS3/latest/userguide/delete-bucket.html) to empty and delete the S3 bucket the CDK has created.\n",
    "\n",
    "You can find the bucket in name the file `cdk_outputs.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bd9589",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat cdk_outputs.json"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
