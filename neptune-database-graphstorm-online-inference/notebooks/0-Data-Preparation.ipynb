{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare graph data\n",
    "\n",
    "### Use 'gsf' kernel\n",
    "\n",
    "To be able to run through this notebook you will need to use the 'gsf' kernel which comes pre-installed with all the dependencies. We create this kernel as part of the notebook setup during the CDK deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook you will start with creating the graph data you will use throughout this example. First you will convert the raw IEEE CIS data into the format Neptune DB and GraphStorm expect for loading, then prepare a configuration file that describes the graph data so they can be processed and ingested by GraphStorm. \n",
    "\n",
    "\n",
    "In the next notebook you will then load this dataset into NeptuneDB.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Set log level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, force=True)\n",
    "logging.getLogger(\"boto3\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"botocore\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"urllib3\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"s3transfer\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"sentence_transformers\").setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Data for GraphStorm Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script will process the IEEE CIS data set into graph, which will be used for GNN model training for a node classification task. The same processed data will be imported into Neptune Database for online inference. The data conversion will take around 2 minutes to run, after which you will analyze the produced files to extract the graph schema.\n",
    "\n",
    "Note: The dataset contains approximately 3.5% fraudulent transactions, making it an imbalanced classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the raw data from the SageMaker examples S3 bucket\n",
    "!aws s3 sync \"s3://sagemaker-solutions-us-west-2/Fraud-detection-in-financial-networks/data\" ./input-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the local utility that converts the raw data into a format that's accepted by both NeptuneDB and GraphStorm\n",
    "from graph_data_preprocessor_neptune_db import create_neptune_db_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "GRAPH_NAME = \"ieee-cis-fraud-detection\"\n",
    "AWS_REGION = \"us-east-1\"\n",
    "\n",
    "PROCESSED_PREFIX = f\"./{GRAPH_NAME}\"\n",
    "\n",
    "ID_COLS = \"card1,card2,card3,card4,card5,card6,ProductCD,addr1,addr2,P_emaildomain,R_emaildomain\"\n",
    "CAT_COLS = \"M1,M2,M3,M4,M5,M6,M7,M8,M9\"\n",
    "# Lists of columns to keep from each file\n",
    "COLS_TO_KEEP = {\n",
    "    \"transaction.csv\": (\n",
    "        ID_COLS.split(\",\")\n",
    "        + CAT_COLS.split(\",\")\n",
    "        +\n",
    "        # Numerical features without missing values\n",
    "        [f\"C{idx}\" for idx in range(1, 15)]\n",
    "        + [\"TransactionID\", \"TransactionAmt\", \"TransactionDT\", \"isFraud\"]\n",
    "    ),\n",
    "    \"identity.csv\": [\"TransactionID\", \"DeviceType\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_neptune_db_data(\n",
    "    data_prefix=\"./input-data/\",\n",
    "    output_prefix=PROCESSED_PREFIX,\n",
    "    id_cols=ID_COLS,\n",
    "    cat_cols=CAT_COLS,\n",
    "    cols_to_keep=COLS_TO_KEEP,\n",
    "    num_chunks=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script you just run will create the graph data under `<PROCESSED_PREFIX>` and separate train/validation/test splits of the data under `<PROCESSED_PREFIX>/data_splits`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create GConstruct configuration file from preprocessed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GraphStorm training requires the original graph data to be converted into a binary, partitioned graph representation to support efficient distributed training. GraphStorm provides the GConstruct module and GSProcessing library that can accomplish this on a single instance or distributed respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create the train data for GraphStorm we need to create a JSON file that describes the tabular graph data. An example file can be:\n",
    "\n",
    "```json\n",
    "\n",
    "{\n",
    "    \"nodes\": [\n",
    "        {\n",
    "            \"node_id_col\":  \"nid\",\n",
    "            \"node_type\":    \"paper\",\n",
    "            \"format\":       {\"name\": \"parquet\"},\n",
    "            \"files\":        [\"paper_nodes.parquet\"],\n",
    "            \"features\":     [\n",
    "                {\n",
    "                    \"feature_col\":  \"embedding\"\n",
    "                }\n",
    "            ],\n",
    "            \"labels\": [   \n",
    "                {\n",
    "                    \"label_col\":    \"paper_field\",\n",
    "                    \"task_type\":    \"classification\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    \"edges\": [\n",
    "        {\n",
    "            \"source_id_col\":    \"src\",\n",
    "            \"dest_id_col\":      \"dst\",\n",
    "            \"relation\":         [\"paper\", \"cites\", \"paper\"],\n",
    "            \"format\":           {\"name\": \"parquet\"},\n",
    "            \"files\":            [\"paper_cites_paper_edges.parquet\"]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To create such a configuration for the preprocessed data you can use the `neptune_gs` package from the repository. The package includes a script that analyzes the output, gets user input to clarify relations and features when needed, and creates the GConstruct configuration JSON file. \n",
    "\n",
    "The package is available at the top level of the repository under the `neptune-gs` directory and you should have installed it during the pre-requisites phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create GConstruct config\n",
    "\n",
    "The `create_graphstorm_config` function analyzes the preprocessed data, optionally asks a series of questions to determine the desired graph schema, and creates a JSON file GConstruct will use as input. The default filename is `gconstruct_config.json`.\n",
    "\n",
    "The program will iterate through all columns for all vertex and relation files, and provide default transformations for each column, provided on whether it's a feature or label.\n",
    "\n",
    "In case it's not able to automatically determine some feature type or edge triplets it will ask for input by the user.\n",
    "\n",
    " > NOTE: We have named the edge files in a way that the files correspond to edge triplets, e.g. `Transaction,identified_by,Card1` fully determines an edge triple. The script relies on this setup to automatically detect edge triples without user input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "from create_gconstruct import create_graphstorm_config\n",
    "\n",
    "GRAPH_DATA_PATH = osp.abspath(PROCESSED_PREFIX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRAPH_DATA_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_config = create_graphstorm_config(\n",
    "    GRAPH_DATA_PATH,\n",
    "    learning_task=\"classification\",  # The task is node classification\n",
    "    target_type=\"Transaction\",  # The target node type are Transaction nodes\n",
    "    label_column=\"isFraud:Int\",  # The property of the Transaction nodes we want to predict. Column type (Int) is appended by Neptune during CSV export\n",
    "    masks_prefix=osp.join(\n",
    "        GRAPH_DATA_PATH, \"data_splits\"\n",
    "    ),  # The location of the train/validation/test masks\n",
    "    cols_to_keep_dict={  # Select a subset of the Transaction properties to include\n",
    "        \"Transaction\":\n",
    "        # Required columns\n",
    "        [\n",
    "            \"~id\",\n",
    "            \"~label\",\n",
    "            \"isFraud:Int\",\n",
    "        ]\n",
    "        +\n",
    "        # Numerical features without missing values\n",
    "        [f\"C{idx}:Float\" for idx in range(1, 15)]\n",
    "        + [\"TransactionAmt:Float\"]\n",
    "        +\n",
    "        # Categorical features\n",
    "        [f\"{CAT_COL}:String\" for CAT_COL in CAT_COLS.split(\",\")]\n",
    "    },\n",
    "    add_reversed_edges=True,  # Add a reverse edge for every edge type\n",
    "    verbose=True,\n",
    "    aggregate_features=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you provide all values the function returns a GraphStorm graph construction configuration dict which you can save to a JSON file. For more information see the documentation about [how GraphStorm performs graph construction](https://graphstorm.readthedocs.io/en/latest/cli/graph-construction/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the config dict in a readable format\n",
    "from pprint import pp\n",
    "\n",
    "pp(gs_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting feature list from GConstruct to use during training\n",
    "\n",
    "\n",
    "To include node/edge features during training and inference, GraphStorm needs a list of all node/edge types with features and the feature names. \n",
    "For this example we are providing a YAML file that already contains all the configuration needed to run a node classification task.\n",
    "\n",
    "If you were writing the YAML file yourself however, you can use \n",
    "the following convenience function to extract the lists of features\n",
    "for every node type to include in your yaml file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from create_gconstruct import extract_features_from_gconstruct_config\n",
    "\n",
    "node_feature_lists, edge_feature_lists = extract_features_from_gconstruct_config(\n",
    "    gs_config\n",
    ")\n",
    "for feature in sorted(node_feature_lists):\n",
    "    print(f\"- {feature}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write graph construction configuration file\n",
    "\n",
    "To be able to build a partitioned graph from the graph data you need to save the configuration dictionary as a JSON file, under the same path as the processed data. \n",
    "\n",
    "`neptune_gs` provides the `FileSystemHandler` class for easier reading and writing files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os.path as osp\n",
    "\n",
    "from fs_handler import FileSystemHandler\n",
    "\n",
    "fs_handler = FileSystemHandler(GRAPH_DATA_PATH)\n",
    "CONFIG_FILENAME = \"ieee-cis-gconstruct-node-classification.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the config locally and at the input location\n",
    "with open(CONFIG_FILENAME, \"w\") as f:\n",
    "    json.dump(gs_config, f, indent=2)\n",
    "\n",
    "with fs_handler.pa_fs.open_output_stream(\n",
    "    f\"{osp.join(GRAPH_DATA_PATH, CONFIG_FILENAME)}\"\n",
    ") as f:\n",
    "    f.write(json.dumps(gs_config, indent=2).encode(\"utf-8\"))\n",
    "\n",
    "print(\n",
    "    f\"GRAPH_NAME: {GRAPH_NAME}\"\n",
    "    f\"\\nConfiguration written to ./{CONFIG_FILENAME}\"\n",
    "    f\"\\nand to {osp.join(GRAPH_DATA_PATH, CONFIG_FILENAME)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run GConstruct to prepare data for training\n",
    "\n",
    "Before being able to use the graph data to train a GraphStorm model, you need to convert the data into a binary, distributed graph format that's compatible with GraphStorm.\n",
    "\n",
    "GraphStorm provides the GConstruct module that takes your input data in CSV/Parquet format, applies feature transformations and converting string IDs to numerical node IDs and saves a partitioned binary representation of the graph that's ready to be used for training.\n",
    "\n",
    "This process also saves metadata that can be used during inference, e.g. information about how to re-apply at inference time the transformations that were applied during training, like one-hot encoding categorical data, or min-max normalization of numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# We need to use the python executable of the gsf kernel\n",
    "PYTHON = sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%cd {GRAPH_DATA_PATH}\n",
    "!{PYTHON} -m graphstorm.gconstruct.construct_graph \\\n",
    "          --conf-file  ieee-cis-gconstruct-node-classification.json \\\n",
    "          --output-dir ../ieee_gs \\\n",
    "          --num-parts 1 \\\n",
    "          --graph-name ieee-cis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "In the next notebook, `1-Load-Data-Into-Neptune-DB.ipynb` you will load the pre-processed CSV data into NeptuneDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gsf",
   "language": "python",
   "name": "gsf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
