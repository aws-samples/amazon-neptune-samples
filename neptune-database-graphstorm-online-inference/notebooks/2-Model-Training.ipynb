{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f9b25bb-140f-4bac-a329-0a43b40f276c",
   "metadata": {},
   "source": [
    "## GraphStorm Model Training \n",
    "\n",
    "\n",
    "After constructing the binary partitioned graph using GConstruct, you can run GraphStorm locally to train a model for detecting fraud transactions. \n",
    "\n",
    "GraphStorm supports model training across multiple environments, including local single-instance training, and distributed training on Sagemaker. \n",
    "\n",
    "For quick proof-of-concept and experiments with smaller graphs, you can train a model locally. Production deployments with enterprise-scale graph data can leverage cluster environments, including Amazon SageMaker, AWS Batch, and Amazon EC2 clusters. This notebook demonstrates local model training on a single instance.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fad797-2ac9-42b6-9dd5-01b38c80b10e",
   "metadata": {},
   "source": [
    "### Model training configuration YAML file\n",
    "\n",
    "GraphStorm enables [model training and inference configurations](https://graphstorm.readthedocs.io/en/latest/cli/model-training-inference/configuration-run.html#model-training-and-inference-configurations) through YAML files and CLI arguments. \n",
    "\n",
    "This allows you to define baseline configurations in a YAML file and use CLI arguments to modify or extend these settings. For this node classification model, we have prepared a baseline configuration file named `ieee_nc.yaml`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa2bd82-d847-426e-8aa9-4f58cbc4d16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ieee_nc.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028e6c8e-4099-4091-ba34-1a50cbfa9da3",
   "metadata": {},
   "source": [
    "The YAML configuration file defines several key configuration categories, including:\n",
    "\n",
    "- GNN Model parameters specify model architecture parameters like ``model_encoder_type``, ``num_layers``, and ``hidden_size``. In this configuration, we define a two-layer [RGCN model](https://arxiv.org/pdf/1703.06103) with a 128-dimensional hidden layer. The ``node_feat_name`` parameter identifies the input features for the model.\n",
    "- Hyperparameter Configurations control the training pipeline's core settings. These include learning rate(``lr``), batch size(``batch_size``), and training epochs(``num_epochs``). Specifically, we set the learning rate to 0.001 , 1024 samples per batch, and provide a CLI argument to train the model for 2 epochs.\n",
    "- Node Classification Configurations define required classification parameters such as target node type (``target_ntype``), label field(``label_field``), and number of classes(``num_classes``). These settings are necessary to set up the classification model.\n",
    "\n",
    "To get started with training your own model, you can use one of the [GraphStorm training configuration examples](https://github.com/awslabs/graphstorm/tree/main/training_scripts) as a starting point, then run a [SageMaker HPO](https://graphstorm.readthedocs.io/en/latest/cli/model-training-inference/distributed/sagemaker.html#launch-hyper-parameter-optimization-task) job to find the best performing set of hyperparameters for your particular problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061e1c23-9bec-4f63-a45e-90e6f5d80ada",
   "metadata": {},
   "source": [
    "### Model Training Commands\n",
    "\n",
    "With the baseline configuration established, you can execute the node classification command to evaluate the initial model performance. Training for 2 epochs should take around 4 minutes on an `ml.m5.4xlarge` notebook instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112d6b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use sys.executable to refer to the gsf kernel's python binary\n",
    "import sys\n",
    "\n",
    "PYTHON = sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e9e352-4851-42e0-b953-b70fc48b65f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!{PYTHON} -m graphstorm.run.gs_node_classification \\\n",
    "           --workspace ./ \\\n",
    "           --part-config ieee_gs/ieee-cis.json \\\n",
    "           --num-trainers 1 \\\n",
    "           --cf ieee_nc.yaml \\\n",
    "           --eval-metric roc_auc \\\n",
    "           --save-model-path ./model-simple/ \\\n",
    "           --topk-model-to-save 1 \\\n",
    "           --num-epochs 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8c4340",
   "metadata": {},
   "source": [
    "The best-performing model has been saved in the `./model-simple` folder, identified by its epoch number (`epoch-01`). In addition, two additional files are saved under this folder, which are needed during the deployment of real-time inference endpoint, `data_transform_new.json` which contains information about the graph data structure, and `GRAPHSTORM_RUNTIME_UPDATED_TRAINING_CONFIG.yaml` which contains information about the GNN model.\n",
    "\n",
    "During inference we use the combined information from these files to re-create the model and re-apply any data transformations online.\n",
    "\n",
    "Finally, we will save a small JSON file here that will tell us where we saved the model, we will re-use this in the following notebook when deploying the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5838a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "with open(\"task_config.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    task_config = {\"MODEL_PATH\": os.path.abspath(\"./model-simple\")}\n",
    "    json.dump(task_config, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf9138e-93df-4076-ba0b-91cca1c2a0bd",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "#### [Optional] Addressing Label Imbalance in training\n",
    "\n",
    "Remember for our dataset, only 3.5% of transaction labels are positive (1), while the majority are negative (0). So using an evaluation metric like simple accuracy would have been misleading, a simple majority classifier can achieve accuracy of ~97%.\n",
    "\n",
    "To ensure fair evaluation, we used the Area Under the Receiver Operating Characteristic Curve [(AUC of ROC)](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) as the evaluation metric. This setting however only affects the evaluation and not the training process.\n",
    "\n",
    "To take the label imbalance into consideration during the training process, we can also assign separate weights to each class, over-weighting the positive class to give more importance to the fraudulent cases in the dataset.\n",
    "\n",
    "The updated command below incorporates new configuration arguments, enabling more robust model training and saving the top-1 performing model for potential real-time inference deployment. \n",
    "\n",
    "While you have a model that is ready to be deployed, you can let the more accurate model to train in the background while you work on deploying the initial model. If you want, you can come back and deploy the more accurate model later.\n",
    "\n",
    "<div style=\"background-color: #fff8e6; color: #994d00; padding: 10px; border-left: 4px solid #994d00; margin-bottom: 10px;\">\n",
    "<strong>Note:</strong> Depending on the instance type, this training process may take minutes to hours.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ed9eee-a9ff-473b-ad44-4d83545b72cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Optional] Start a long-running training command and move on to the next notebook\n",
    "!{PYTHON} -m graphstorm.run.gs_node_classification \\\n",
    "           --workspace ./ \\\n",
    "           --part-config ieee_gs/ieee-cis.json \\\n",
    "           --num-trainers 1 \\\n",
    "           --cf ieee_nc.yaml \\\n",
    "           --model-encoder-type hgt \\\n",
    "           --num-ffn-layers-in-gnn 2 \\\n",
    "           --num-heads 16 \\\n",
    "           --hidden-size 128 \\\n",
    "           --eval-metric roc_auc \\\n",
    "           --imbalance-class-weights 0.1,1.0 \\\n",
    "           --fanout 10,10 \\\n",
    "           --save-model-path model-advanced/ \\\n",
    "           --topk-model-to-save 1 \\\n",
    "           --num-epochs 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6feb9c",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "With the artifacts from the simple model available under `./model-simple` you can proceed to notebook `3-GraphStorm-Endpoint-Deployment.ipynb`, where you will deploy the trained model and prepare for online GNN inference."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gsf",
   "language": "python",
   "name": "gsf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
